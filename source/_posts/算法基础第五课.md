---
title: 算法基础第五课
date: 2019-02-24 14:40:57
categories:
  - leetcode
---

### 认识哈希函数和哈希表

<br/>

#### 性质

1. 输入域无穷大
2. 输出域有穷大
3. 输入固定，则输出固定。
4. 输入不固定，输出也有可能一致，哈希碰撞
5. 保证**离散性**，均匀分布，例如 输入0-98 ，输出0，1，2，给出0-98 99个不同的数字，那么最终的结果会趋近于0有33个，1有33个，2有33个，这就是哈希函数的**离散性**： 虽然不同的输入也可能导致相同的输出，但是如果给定很多不同的输入，那么最终输出的固定S域中，也会均匀分布

<br/>

#### 特征：

1. 和输入没有规律，即使输入有规律(很相近)，哈希之后可能相差很远。
2. 推广：input通过hash函数之后得到一个code，那么最终所有的code 都 % m之后，也会均匀的散列在m-1上。如果所有input在哈希函数之后的S域上，是均匀分布的，那么在对m取模之后，在m-1上也是均匀分布的
3. 增删改查都是O(1)

<br/>

#### 大数据应用

大文件，每一行是一个字符串，打印所有重复的字符串。100T
1000台机器，读取每一行字符串，利用hash函数计算值，再%1000，把每一行的字符串都分流到1000台机器，而且相同的字符串肯定都会到相同的机器。假设有m种不同的字符串，这m种不同的字符串就会均匀的分布到1000台机器上。
在每台机器上统计重复的字符串。

{% asset_img picture1.jpg %}

<br/>

### 设计RandomPool结构

设计一种结构，在该结构中有如下三个功能： 

insert(key)：将某个key加入到该结构，做到不重复加入。 

delete(key)：将原本在结构中的某个key移除。 

getRandom()： 等概率随机返回结构中的任何一个key。

<br/>

思路：

一个哈希表肯定不行，因为小样本的时候，分布肯定不均匀，离散型只是保证大样本的时候，近似均匀；

准备两个map；设置一个size大小，进来一个元素A，size+1，插入map1：key：A，value：0，插入map2：key：0，value：A，之后进来的数都以这种方式存储；假如现在size=26，取数的时候，使用Random 随机从0到25上取一个，用取来的随机数作key，从map2中取出来value；就是完全随机的一个字母key。因为map中存储的就是以index作为key，字母key做value的一个map。size代表了0到26上都是有数字的。但是这是在没有remove的情况下，

{% asset_img picture2.jpg %}

<br/>

remove时候的做法：

如果单纯的两个map remove，中间会产生洞，随机取数就会有问题。因为下次取得可能就是上次被remove的问题；
每次remove的时候，用最后一条记录覆盖要删除的记录；
然后把最后一条记录删除，size-1；
这样就会避免产生中间的洞。保证size上random 都是随机的；

<br/>

代码：

```java
public class Code_02_RandomPool {
	public static class Pool<K> {
		private HashMap<K, Integer> keyIndexMap;
		private HashMap<Integer, K> indexKeyMap;
		private int size;

		public Pool() {
			this.keyIndexMap = new HashMap<K, Integer>();
			this.indexKeyMap = new HashMap<Integer, K>();
			this.size = 0;
		}

		public void insert(K key) {
			if (!this.keyIndexMap.containsKey(key)) {
				this.keyIndexMap.put(key, this.size);
				this.indexKeyMap.put(this.size++, key);
			}
		}

		public void delete(K key) {
			if (this.keyIndexMap.containsKey(key)) {
				int deleteIndex = this.keyIndexMap.get(key);
				int lastIndex = --this.size;
				K lastKey = this.indexKeyMap.get(lastIndex);
				this.keyIndexMap.put(lastKey, deleteIndex);
				this.indexKeyMap.put(deleteIndex, lastKey);
				this.keyIndexMap.remove(key);
				this.indexKeyMap.remove(lastIndex);
			}
		}

		public K getRandom() {
			if (this.size == 0) {
				return null;
			}
			int randomIndex = (int) (Math.random() * this.size); // 0 ~ size -1
			return this.indexKeyMap.get(randomIndex);
		}

	}

	public static void main(String[] args) {
		Pool<String> pool = new Pool<String>();
		pool.insert("zuo");
		pool.insert("cheng");
		pool.insert("yun");
		System.out.println(pool.getRandom());
		System.out.println(pool.getRandom());
		System.out.println(pool.getRandom());
		System.out.println(pool.getRandom());
		System.out.println(pool.getRandom());
		System.out.println(pool.getRandom());
	}
}
```

<br/>

### 布隆过滤器

问题：

黑名单问题：

100亿个url，是一个黑名单，每个连接64字节，
给一个url，返回true或false，即判断是否在黑名单中；
如果使用hashset，起码需要6400亿个字节空间才能装下，640G。
布隆过滤器就是一个集合，可以查询某个元素是否存在，
但是它有失误率(一个url不再黑名单中，但是仍然返回了true)
宁可错杀3000，不放过1个。但是不会url是黑名单却返回false的情况。

<br/>

#### bloomfilter

一个bit类型的数组，可以用int等基本类型实现。
一个url，经过k个hash函数，经过每个hash函数处理后都要再对其 % m （m是这个bit数组的长度），把这k个位置上的bit位都描黑，所以，准备的bit类型的数组要足够大，否则，就都描黑了。
查询一个url在不在黑名单中，也要经过key个函数，哈希完后再对其 % m ，查看得出的位置的颜色，如果bit数组中的这k个位置上都描黑了 ，说明就是黑名单，只要有一个位置没有描黑，那么一定不在黑名单中。

<br/>

#### 直观感受

我们可以通过控制bit数组的长度和 哈希函数 的个数来控制失误率

bit数组越长，失误率越小，哈希函数的个数越多，失误率越小；

<br/>

### 参数调试

bit数组的长度m= - ( n*ln(p) ) / ( ln(2) ) ^2    

n：样本量

p：预期失误率

<br/>

函数函数的个数 k =  ln(2) * ( m/n ) = 0.7 * ( m/n )

<br/>

真实失误率 = 

{% asset_img picture3.jpg %}

<br/>

#### int类型数组实现bitmap的代码

```java
public class Test {

    public static void main(String[] args) {

        int[] arr = new int[1000];  //32000个bit位。

        int index = 30000;          //把第30000个bit位描黑

        int intIndex = index / 32;  //位于第932个桶（位置）上。就是int 数组第932个index上

        int bitIndex = index % 32;  //第932个int位置上的第16个bit位上。一个int有32个bit位

        //第932个桶中的第16个bit位置应该被描黑

        // 1 左移 bitIndex 就表示32个bit位置， 只有 第bitIndex 位置为1，其他31位都是0，

        arr[intIndex] = (arr[intIndex] | (1 << bitIndex));//(1 << bitIndex)表示第16个位置上是1，其他都是0，
        //(arr[intIndex]  或  (第16个位置上是1 其他都是0的二进制)  则原bit位第16个bit位原位置就变黑了。
    }
}
```

<br/>

<br/>

### 一致性哈希

<br/>

#### 本质：

就是一种服务器的设计；

<br/>

#### 经典服务器抗压结构

前端3台服务器各自带一个相同的hash函数，存储string的请求到前台一个服务器上，经过一个hash函数再%后台的机器数(假设是3)，如果结果是1，那么数据会存储在后台1这个服务器上。按照这种结构，所有的string就会均匀的分散在后台存储的3台服务器上。

<br/>

#### 问题

加机器，再hash，位于的机器就变化了。

<br/>

#### 解决办法

一致性哈希：降低数据迁移的代价，同时又负载均衡，

<br/>

假设hash之后的范围的取值范围是一个环，把后台机器m1,m2,m3的ip通过相同的hash函数算出一个hash值，对应到函数的一个位置，把三台机器对应到环上去，使用这种结构提供服务，计算一个string通过hash函数的值，此时不再取模，这个值会打到hash环的一个位置，就顺时针找到距这个string的hash最近的服务器，放到这台服务器上，查询的时候也如此。

{% asset_img picture4.jpg %}

<br/>

#### 实现

前台的每台机器内存中都存放一个数组：数组中是每台机器按照hash值拍好序的，存储一个string，使用同一个函数函数取得hash值，使用二分的方式，取得数组中从左到右第一个 >= 此hash值的机器的hash值，就放在这台机器上。

{% asset_img picture5.jpg %}

<br/>

此时加机器减机器都只需要移动新添机器和逆时针上一个节点具体之间的数据。
数据迁移的代价就很低。

{% asset_img picture6.jpg %}

<br/>

问题：

机器数很少，那么两个机器打到环上的距离可能很近，此时负载就非常不均衡，哈希函数在大样本的情况下才能保证均匀性。

{% asset_img picture7.jpg %}

<br/>

解决：

虚拟节点技术：m1,m2,m3各自虚拟成1000个虚拟节点，把虚拟节点hash之后放在环上，按照路由表，m1的1000个虚拟节点放在物理机器m1上，m2,m3 也如此。 

<br/>

<br/>

### 并查集

<br/>

用途

1. 给定两个元素，判断两个元素各自所在的集合，是否是一个集合
2. 把两个集合合并为一个集合

<br/>

结构的猜想

如果用List实现，合并集合的时候可以直接把两个List相连，但是判断是否是同一个集合的时候，需要遍历

如果用Set实现，判断是否是同一个集合的过程，可以直接在A的集合中查看是否有B元素，但是合并集合的时候需要遍历，把B中的所有元素遍历扔到A中；

所以这两种方式都是不够好的；

<br/>

真实的结构：

每个集合都有自己的代表节点，一个元素的代表节点就是通过一个它的父指针，一直向上指，直到找到一个元素的父指针指向了自己，这个元素就是这个集合的代表节点；

实际就是一个多叉树

判断两个元素是否属于同一个集合，就看两个元素所在集合的代表节点是否是同一个节点；

{% asset_img picture8.jpg %}

<br/>

两个元素所在集合的合并就是，找到两个元素所在集合的代表代表节点，然后看哪个集合的节点个数少，把少的挂在节点数多的集合的代表节点下面

{% asset_img picture9.jpg %}

<br/>

优化：

在查询一个元素所在集合的代表节点的时候，一直往上查的过程，查完之后，把这条路径上的所有元素打扁平，就是把这个元素之上的元素统一挂在代表节点的下一层，在集合合并和查询是否是同一个集合的过程中都会使用这个优化（只要查就优化）

{% asset_img picture10.jpg %}

<br/>

优化的过程代码：

```java
//给定一个元素，向上找代表节点，过程中变扁平的行为
private Node findHead(Node node) {
    Node father = fatherMap.get(node);
    if (father != node) {
        father = findHead(father);   //递归的向上找，直到找到代表节点。最后返回代表节点，然后再回退之前走的路径上的每个节点，
        // 回退的过程中，每次都返回代表节点，把每个节点的父节点都变成代表节点，也就是优化变扁平的过程；
    }
    fatherMap.put(node, father);
    return father;
}

// 迭代版本
private Node findHead1(Node node) {
    Stack<Node> stack = new Stack<Node>();

    Node cur = node;
    Node patent = fatherMap.get(cur);
    while (cur != patent){
        stack.push(cur);
        cur = patent;
        patent = fatherMap.get(cur);
    }

    while (!stack.isEmpty()){
        fatherMap.put(stack.pop(),patent);
    }
    return patent;
}
```

图示：

{% asset_img picture11.jpg %}

<br/>

时间复杂度

证明略，

只要查询次数+合并次数 逼近 O(n) 及以上，那么平均下来单次查询的时间复杂度为 O(1)

<br/>

代码：

```java
public class Code_04_UnionFind {

	public static class Node {
		// whatever you like
	}

	public static class UnionFindSet {
		public HashMap<Node, Node> fatherMap;//key：某一个节点，value：节点的父节点
		public HashMap<Node, Integer> sizeMap;//某一个节点所在的集合有多少个节点。

		public UnionFindSet(List<Node> nodes) { // 只接受一次把全部的样本都给我，不接受一个一个的进来
			makeSets(nodes);
		}

		public void makeSets(List<Node> nodes) {
			fatherMap = new HashMap<Node, Node>();
			sizeMap = new HashMap<Node, Integer>();
			fatherMap.clear();
			sizeMap.clear();
			for (Node node : nodes) {
				fatherMap.put(node, node);  // 开始的时候都是一个元素就是一个集合，父指针指向自己
				sizeMap.put(node, 1); //每一个节点都是自己的代表节点
			}
		}

		//给定一个元素，向上找代表节点，过程中变扁平的行为
		private Node findHead(Node node) {
			Node father = fatherMap.get(node);
			if (father != node) {
				father = findHead(father);   //递归的向上找，直到找到代表节点。最后返回代表节点，然后再回退之前走的路径上的每个节点，
									// 回退的过程中，每次都返回代表节点，把每个节点的父节点都变成代表节点，也就是优化变扁平的过程；
			}
			fatherMap.put(node, father);
			return father;
		}

		// 迭代版本
		private Node findHead1(Node node) {
			Stack<Node> stack = new Stack<Node>();

			Node cur = node;
			Node patent = fatherMap.get(cur);
			while (cur != patent){
				stack.push(cur);
				cur = patent;
				patent = fatherMap.get(cur);
			}

			while (!stack.isEmpty()){
				fatherMap.put(stack.pop(),patent);
			}
			return patent;
		}

		//对外功能
		public boolean isSameSet(Node a, Node b) {
			return findHead(a) == findHead(b);
		}

		//对外功能
		public void union(Node a, Node b) {
			if (a == null || b == null) {
				return;
			}
			Node aHead = findHead(a);
			Node bHead = findHead(b);
			if (aHead != bHead) {
				int aSetSize= sizeMap.get(aHead);  // 获取一个集合的size的时候，只通过head代表节点去获取的
				int bSetSize = sizeMap.get(bHead);
				if (aSetSize <= bSetSize) {
					fatherMap.put(aHead, bHead);	//把A的头结点指向B集合的head，就是把A集合挂在B集合上
					//修改size的时候，只需要修改头结点的size就可以，因为得到size的时候，就是通过代表节点作为key去获取的
					sizeMap.put(bHead, aSetSize + bSetSize);  // B集合的size变大
				} else {
					fatherMap.put(bHead, aHead);
					sizeMap.put(aHead, aSetSize + bSetSize);
				}
			}
		}
	}
}
```

<br/>

### 岛问题：

一个矩阵中只有0和1两种值，每个位置都可以和自己的上、下、左、右 四个位置相连，如果有一片1连在一起，这个部分叫做一个岛，求一个 矩阵中有多少个岛？

<br/>

举例：

0 0 1 0 1 0 

1 1 1 0 1 0 

1 0 0 1 0 0

0 0 0 0 0 0

这个矩阵中有三个岛。

<br/>

单个cpu解答

```java
public class Code_03_Islands {
    public static int countIslands(int[][] m) {
        if (m == null || m[0] == null) {
            return 0;
        }
        int N = m.length;
        int M = m[0].length;
        int res = 0;
        for (int i = 0; i < N; i++) {
            for (int j = 0; j < M; j++) {
                if (m[i][j] == 1) {
                    res++;   // 岛个数+1
                    infect(m, i, j, N, M); // 感染
                }
            }
        }
        return res;
    }

    // 感染函数，递归的感染一个数的上下左右，上下左右再继续感染；
    public static void infect(int[][] m, int i, int j, int N, int M) {
        if (i < 0 || i >= N || j < 0 || j >= M || m[i][j] != 1) {   // 没有越界，并且当前节点不是1，停止感染
            return;
            // 如果是2或是0，直接返回了
        }
        m[i][j] = 2;   //当前点感染为2
        infect(m, i + 1, j, N, M);  //感染上
        infect(m, i - 1, j, N, M);  //感染下
        infect(m, i, j + 1, N, M);  //感染左
        infect(m, i, j - 1, N, M);  //感染右
    }

    public static void main(String[] args) {
        int[][] m1 = {  { 0, 0, 0, 0, 0, 0, 0, 0, 0 },
                { 0, 1, 1, 1, 0, 1, 1, 1, 0 },
                { 0, 1, 1, 1, 0, 0, 0, 1, 0 },
                { 0, 1, 1, 0, 0, 0, 0, 0, 0 },
                { 0, 0, 0, 0, 0, 1, 1, 0, 0 },
                { 0, 0, 0, 0, 1, 1, 1, 0, 0 },
                { 0, 0, 0, 0, 0, 0, 0, 0, 0 }, };
        System.out.println(countIslands(m1));

        int[][] m2 = {  { 0, 0, 0, 0, 0, 0, 0, 0, 0 },
                { 0, 1, 1, 1, 1, 1, 1, 1, 0 },
                { 0, 1, 1, 1, 0, 0, 0, 1, 0 },
                { 0, 1, 1, 0, 0, 0, 1, 1, 0 },
                { 0, 0, 0, 0, 0, 1, 1, 0, 0 },
                { 0, 0, 0, 0, 1, 1, 1, 0, 0 },
                { 0, 0, 0, 0, 0, 0, 0, 0, 0 }, };
        System.out.println(countIslands(m2));
    }
}
```

<br/>

假设矩阵巨大无比，但是我们有多个cpu，多台机器，尝试用分治的思路解决；

关键点在于：只要收集完边界信息，就足以判断需要减去几个了（从分治后算出来的总的），但是要注意合并边界信息：不能多减去，如下图，不能减2，只能减1

{% asset_img picture12.jpg %}

<br/>

边界信息到底该如何合并？

{% asset_img picture13.jpg %}

查看感染源是不是同一个代表节点；

两个中心和在一起是什么意思， 就是因为某些拓扑结构的联系，两个岛和在一起了，重复合的岛是不应该重复减1的，这样会让岛的数量出错，所以这个场景用并查集非常合适。因为连成一片，天然就可以使用并查集这个结构；

<br/>

在结构上怎么避免重复的已经合完的部分不要重复减这个问题，用并查集来解决它；

{% asset_img picture14.jpg %}