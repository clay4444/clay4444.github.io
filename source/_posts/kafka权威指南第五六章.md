---
title: kafka权威指南第五六章
categories:
  - kafka权威指南读书笔记
abbrlink: f63f9c6e
date: 2019-05-18 23:15:24
---

## 第五章：深入kafka

1. Kafka 如何进行复制；
2. Kafka 如何处理来自生产者和消费者的请求
3. Kafka 的存储细节， 比如文件格式和索引。

<br/>

### 集群成员管理

Kafka使用Zookeeper管理集群成员状态，每一个broker都有一个唯一ID（在配置文件中指定或者自动生成），当broker启动时会在Zookeeper中注册相应的临时节点。如果集群中存在相同的ID，那么新的broker会启动失败。

Zookeeper中的节点注册路径为/broker/ids，Kafka的各个组件会监听此路径下的变更信息，当broker加入或者离开时，它们会收到通知。当节点离开（可能由于停机、网络故障、长GC等导致）时，Zookeeper中相应的节点会消失，但该broker的ID仍然会在某些数据结构中存在。比如，每个主题的副本列表会包含副本所在的broker ID，因此如果一个broker离开同时有一个新的broker拥有此相同的ID，那么新的broker会在集群中替代之前的broker，并且会被分配同样的主题和分区。

<br/>

### 集群控制器（Controller）

集群控制器也是一个broker，在承担一般的broker职责外，它还负责选举分区的主副本（后面会讨论分区主副本的作用）。集群中的第一个broker通过在Zookeeper的/controller路径下创建一个临时节点来成为控制器。当其他broker启动时，也会试图创建一个临时节点，但是会收到一个“节点已存在”的异常，这样便知道当前已经存在集群控制器。这些broker会监听Zookeeper的这个控制器临时节点，当控制器发生故障时，该临时节点会消失，这些broker便会收到通知，然后尝试去创建临时节点成为新的控制器。

对于一个控制器来说，如果它发现集群中的一个broker离开时，它会检查该broker是否有分区的主副本，如果有则需要对这些分区选举出新的主副本。控制器会在分区的副本列表中选出一个新的主副本，并且发送请求给新的主副本和其他的跟随者；这样新的主副本便知道需要处理生产者和消费者的请求，而跟随者则需要向新的主副本同步消息。

如果控制器发现有新的broker（这个broker也有可能是之前宕机的）加入时，它会通过此broker的ID来检查该broker是否有分区副本存在，如果有则通知该broker向相应的分区主副本同步消息。

最后，每当选举一个新的控制器时，就会产生一个新的更大的控制器时间戳（controller epoch），这样集群中的broker收到控制器的消息时检查此时间戳，当发现消息来源于老的控制器，它们便会忽略，以避免脑裂（split brain）。

<br/>

### 复制

 复制，是Kafka架构的核心。通过复制机制，Kafka达到了高可用的要求，保证在少量节点发生故障时集群仍然可用。如前所述，每个主题都有若干个分区，而每个分区有多个副本，这些副本都存在broker中。Kafka中的副本有两种类型：

- 主副本（leader replica）：每个分区都有唯一的主副本，所有的生产者和消费者请求都由主副本处理，这样才能保证一致性。
- 跟随者副本（follower replica）：分区的其他副本为跟随者副本，跟随者副本不处理生产者或消费者请求，它们只是向主副本同步消息，当主副本所在的broker宕机后，跟随者副本会选举出新的主副本。

 对于主副本来说，它还有一个职责，那就是关注跟随者副本的同步状态。每个跟随者副本都会保持与主副本同步，但在异常情况（例如网络阻塞、机器故障、重启等）下，跟随者的状态可能会同步失败。跟随者副本通过向主副本发送Fetch请求来进行同步（与消费者一样），请求中包含希望同步的下一个消息位移，该位移始终是有序的。比如，一个跟随者可能会按序请求消息1，消息2，消息3…如果跟随者请求消息N，那么主副本可以确定此跟随者已经接收到N-1及以前的消息了。因此根据请求中的位移信息，主副本知道跟随者的落后状态，如果副本超过10秒钟没有发送同步请求或者请求的位移属于10秒钟以前的位移，那么主副本会认为该跟随者是同步落后（out of sync）的。如果一个跟随者副本是同步落后的，那么在主副本发生故障时该跟随者不能成为新的主副本。而能够及时同步的跟随者副本则是in-sync状态的，这些跟随者副本有资格成为新的主副本。当然10秒钟不是固定的，可以通过replica.lag.time.max.ms来设置。

每个分区除了有主副本之外，还有一个备选主副本（preferred leader），它是主题初始创建时的主副本。在主题初始创建时，Kafka使用一定的算法来分散所有主题的主副本与跟随者副本，因此备份主副本通常能保证集群的流量是均衡分布的。如果备份主副本是in-sync状态的，那么在主副本发生故障后，它会自动成为新的主副本。

<br/>

### 请求处理

 Kafka的broker主要工作是，当作为分区的主副本时，处理来自生产者/消费者客户端、跟随者副本以及控制器的请求。对于此Kafka使用自定义的二进制协议来进行通信，请求的头部包含如下信息：

- 请求类型
- 请求版本（通过此参数broker可以正确处理不同版本客户端的请求）
- 关联ID：标识请求的全局唯一数字，也会在响应结果和错误日志中呈现。
- 客户端ID：用来识别请求的来源。

同一个客户端发送的所有请求都是按序处理的，这保证了消息的有序性。

对于监听的每一个端口，broker都会运行一个接收者（acceptor）线程来建立连接，并且把连接交给一个处理器（processor）线程处理，处理器线程的数量是可配置的。处理器线程接收客户端的请求，把请求放进请求队列，最后从响应队列中取出结果返回给客户端。当请求放进请求队列后，IO线程负责进行处理，大部分的请求都属于两种类型：

- 生产请求（produce request）：由生产者发送，包含需要写入的消息。
- 拉取请求（fetch request）：由消费者和跟随者副本发送。

整体架构如下所示：

{% asset_img picture1.jpg %}

生产请求和拉取请求都需要发送给分区的主副本处理，如果一个跟随者副本收到这两种请求，它会返回“Not a Leader for Partition”的异常信息，客户端收到异常信息后向正确的主副本发送请求。

客户端怎么知道哪个是主副本呢？通过使用另一种类型的请求来实现，那就是元信息请求（metadata request）。Kafka的任意一个broker都可以处理这种请求，请求中包含客户端感兴趣的主题列表，broker会返回这些主题的分区列表、分区的副本列表以及主副本信息。客户端收到这些信息后，会进行一定时间的缓存（由metadata.max.age.ms指定），当超过时间或者broker返回请求的异常后，会刷新此信息。整体交互处理如下：

{% asset_img picture2.jpg %}

<br/>

#### 生产请求（produce request）

acks参数控制多少个副本确认写入成功后生产者才认为消息生产成功。这个参数的取值可以为：

- acks=0：消息发送完毕，生产者认为消息写入成功；
- acks=1：主副本写入成功，生产者认为消息写入成功；
- acks=all：所有in-sync副本写入成功，生产者认为消息写入成功。

如果主副本收到生产消息，它会执行一些检查逻辑，包含：

发送的用户是否有权限写入主题？请求的acks参数取值是否合法（只允许0，1，all）？如果acks设置为all，是否有足够的in-sync副本来安全写入消息？（我们可以配置如果in-sync副本低于一定数量，主副本拒绝写入消息）

检查通过后主副本会持久化消息到本地磁盘，在Linux系统上消息只会写入到文件系统的缓存，因此并没有保证一定写入到磁盘；Kafka依赖**复制机制**来保证数据不丢失。

一旦消息本地持久化后，如果acks=1那么会返回结果给客户端，如果acks=all那么会将请求放置在一个称为purgatory的缓冲区中等待其他的副本写入完成。

<br/>

#### 拉取请求（fetch request）

主副本处理拉取请求和处理生产请求差不多。客户端发送一个拉取请求，包含主题、分区和位移信息，主副本返回相应的数据。另外，客户端也指定返回的最大数据量，防止数据量过大造成客户端内存溢出。同时，客户端也指定返回的最小数据量，当消息数据量没有达到最小数据量时，请求会一直阻塞直到有足够的数据返回，如下所示：

{% asset_img picture3.jpg %}

指定最小的数据量在负载不高的情况下非常有用，通过这种方式可以减轻网络往返的额外开销。当然请求也不能永远的阻塞，客户端可以指定最大的阻塞时间，如果到达指定的阻塞时间，即便没有足够的数据也会返回。

有意思的是，不是所有主副本的数据都能够被读取。当数据被所有in-sync状态的副本写入成功后，它才能被客户端读取。对于主副本来说，这并不难实现，因为之前已经说过通过副本同步，它知道所有副本当前已经完成同步的消息位移。该机制描述如下：

{% asset_img picture4.jpg %}

但为什么要这么做呢？这是因为如果没有足够的副本持久化消息，该消息是不安全的。如果主副本发生故障，然后其他副本成为新的主副本，这些消息将会在Kafka中莫名其妙的消失。也就是说，存在一个消费组能够读到某个消息，但另外的消费组读不到这个消息，从而导致不一致的行为。

这样的机制也导致了新消息到达消费者应用的高延迟，特别是存在副本之间网络拥塞的情况。我们可以通过replica.lag.time.max.ms来指定副本在复制消息时可被允许的最大延迟时间，减小该值可以使得消费者无需等待延迟较大的副本写入。

在读取消息上，Kafka使用零复制（zero-copy）来提高性能。也就是说，Kafka将文件（更准确的说，是文件系统缓存）的消息直接传给网络通道，并没有使用中间的buffer。这避免了内存的字节拷贝和buffer维护，极大地提高了性能。

<br/>

#### 其他请求

我们讨论了Kafka中最常见的三种请求类型：元信息请求，生产请求和拉取请求。这些请求都是使用的是Kafka的自定义二进制协议。集群中broker间的通信请求也是使用同样的协议，这些请求是内部使用的，客户端不能发送。比如在选举分区主副本过程中，控制器会发送LeaderAndIsr请求给新的主副本和其他跟随副本。

 这个协议目前已经支持20种请求类型，并且仍然在演进以支持更多的类型。另外，Kafka也会修改已有的请求来增加扩展性。比如在0.9.0版本和0.10.0版本，Kafka决定在元信息请求中返回集群的控制器信息，因此在元信息请求和返回值中增加了一个版本号；0.9.0版本的客户端会发送版本号为0的元信息请求，而broker（无论是0.9.0或者0.10.0）也会返回版本号为0的结果，结果中不会包含控制器信息；而0.10.0版本的客户端会发送版本号为1的元信息请求，0.10.0版本的broker会返回版本号为1的结果，并且包含控制器信息。注意的是，如果0.10.0版本的客户端发送此请求到0.9.0版本的broker，那么请求会报错，这也是为什么在升级时先升级broker再升级客户端的理由。

<br/>

### 物理存储

Kafka中基本的存储单元是一个分区副本。分区副本不能跨越多个broker，甚至不能跨越同一个broker的多个磁盘。当配置Kafka时，管理员需要设置log.dirs来指定分区存储的目录。

下面来看下存储的细节。

<br/>

#### 分区分配

当创建一个新主题时，Kafka首先需要决定如何分配分区到不同的broker。假设有6个broker，那么创建一个有10个分区且复制因子为3的主题的话，Kafka需要分配30个分区副本到这6个broker。分配策略主要的考虑因素如下：

- 尽可能将分区副本均衡分配到集群的broker中；
- 对于每个分区，它的所有副本需要在不同的broker；
- 如果broker（0.10.0及更高版本）有机架信息，那么对于一个分区的所有副本，尽量分配这些副本到不同的机架。这保证了机架发生故障时集群仍然可用。

下面以一个例子来说明这几个策略。假设集群由6个broker，我们从一个随机的broker（不妨称为4号）开始以轮询的方式来分配分区的主副本。因此分区0的主副本在4号broker，分区1的主副本在5号broker，分区2的主副本在0号（总数量为6，下标从0开始）broker…然后对于每个分区的其他副本，从其主副本所在的broker开始分配。由于分区0的主副本在4号broker，分区0的第一个跟随者副本在5号broker，第二个跟随者副本在6号broker，以此类推…

如果考虑机架因素，那么我们需要首先根据broker的机架信息来对broker进行排序，而不是上面的按照序号递增来排序。假如0号、1号和2号broker在同一个机架，3号、4号和5号broker在另一个机架，那么broker的顺序可能为0，3，1，4，2，5。这个顺序穿插了不同机架的broker。那么对于这种情况，分区0的主副本在4号broker，分区1的主副本在2号broker…

在分配分区和副本到broker之后，下一步需要决定使用哪个目录来存储分区。策略非常简单：统计每个目录的分区数，把新分区分配到最少分区数的目录中。因此如果新增加一个磁盘，所有的新分区都会分配到在这个磁盘上，因为它的分区数最少。

<br/>

#### 文件管理

消息保留，是Kafka中一个很重要的概念。Kafka不会永远保留数据，也不会等待所有的消费组读取了消息才删除消息。只要数据量达到上限或者数据达到过期时间，Kafka会删除老的消息数据。

因为在一个大文件中查找需要清理的数据并进行删除是非常耗时而且容易出错的，Kafka将每个分区切割成段（segment）。默认每个段大小不超过1G，且只包含7天的数据。如果段的消息量达到1G，那么该段会关闭，同时打开一个新的段进行写入。

正在写入的段称为活跃段（active segment），活跃段不会被删除。因此，假如设置日志保留时间为1天，但是日志段包含5天的数据，那么我们实际上会保留5天的数据，因为活跃段正在使用而且在关闭前不会删除。

对于每个分区的每个段（包括不活跃的段），broker都会维护文件句柄，因此打开的文件句柄数通常会比较多，这个需要适度调整系统的进程文件句柄参数。

<br/>

#### 文件格式

每个段都单独存为一个文件，文件内存放消息和位移。磁盘上的数据格式、生产者发送的数据格式和消费者读取的数据格式都是一样的，使用相同的数据格式使得Kafka可以进行零拷贝优化，以及避免压缩和解压缩。

除了key/value和位移之外，每个消息还包含消息大小、校验和（检测数据损坏）、魔数（标识消息格式版本）、压缩算法（Snappy、GZip或者LZ4）和时间戳（0.10.0新增）。

如果发送者发送压缩的消息，那么批量发送的消息会压缩在一起，以“包装消息”（wrapper message）来发送，如下所示：

{% asset_img picture5.jpg %}

因此如果生产者使用压缩，那么发送更大的批量消息可以得到更好的网络传输效率，并且节省磁盘存储空间。

<br/>

#### 索引

Kafka允许消费者从任意合法的位移拉取消息，也就是说如果消费者希望从位移为100的地方开始读取1MB的消息，broker需要在该分区的所有段中定位到该消息的位置然后开始读取数据。为了帮助broker迅速定位，Kafka对于每个分区都维护一个索引，该索引将位移映射到段以及段内偏移。

索引也是按照段来切割的，因此清理过期消息时相应的索引也可以很容易清理。另外，索引并没有维护校验和，因此如果索引损坏了，Kafka会重新读取段文件生成索引。

<br/>

#### 压缩（compaction） / 清理

正常情况下，Kafka存储一定数量的消息，并且如果消息超过一定时间，这些消息会被清除。此外，Kafka还支持压缩的消息保留策略，使用这种策略会使得对于主题内的每个键，Kafka只会保留最新的消息内容。显然，压缩的策略对同时包含键和消息内容的主题才生效，如果主题内的消息键为null，那么压缩的策略不会生效。

<br/>

#### 压缩的工作原理：

每个分区都可以分为两部分：

- 干净（clean）：这部分消息之前已经被压缩过，对于每个key来说这部分只存在一个value。
- 脏（dirty）：在上一次压缩后写入的新消息。

如下所示：

{% asset_img picture6.jpg %}

如果使用压缩，那么每个broker会启动一个压缩管理器线程和若干个压缩线程，每个线程负责一个分区。

在压缩分区时，压缩线程会首先读取脏的部分，并且建立一个key的哈希和位移的映射，对于相同的键，只保留最新的位移。其中key的哈希大小为16字节，位移大小为8个字节。也就是说，一个映射只有24字节，假设消息大小为1KB，那么1GB的段有1百万条消息，建立这个段的映射只需要24MB的内存，映射的内存效率是非常高效的。

在配置Kafka时，管理员需要设置这些压缩线程可以使用的总内存。如果设置1GB的总内存同时有5个压缩线程，那么每个线程只有200MB的内存可用。在压缩线程工作时，它不需要把所有脏的段文件都一起在内存中建立上述映射，但需要保证至少能够建立一个段的映射。如果不能同时处理所有脏的段，Kafka会一次压缩最老的几个脏段，然后在下一次再处理其他的脏段。

一旦建立完脏段的键与位移的映射后，压缩线程会从最老的干净的段开始处理。如果发现段中的消息的键没有在映射中出现，那么可以知道这个消息是最新的，然后简单的复制到一个新的干净的段中；否则如果消息的键在映射中出现，这条消息需要抛弃，因为对于这个键，已经有新的消息写入。处理完会将产生的新段替代原始段，并处理下一个段。

对于一个段，处理前和处理后的效果如下：

{% asset_img picture7.jpg %}

<br/>

#### 删除事件

对于只保留最新消息的压缩策略来说，Kafka还支持删除相应键的消息操作（而不仅仅是保留最新的消息内容）。这是通过生产者发送一条特殊的消息来实现的，该消息包含一个键以及一个null的消息内容。当压缩线程发现这条消息时，它首先仍然进行一个正常的压缩并且保留这个包含null的特殊消息一段时间，在这段时间内消费者消费者可以获取到这条消息并且知道消息内容已经被删除。过了这段时间，压缩线程会删除这条消息，这个键会从分区中消失。这段时间是必须的，因为它可以使得消费者有一定的时间余地来收到这条消息。

<br/>

#### 什么时候压缩？

与过期清除的策略一样，压缩策略也不会对活跃段进行压缩。在0.10.0以及更老的版本，Kafka会在主题包含50%脏记录的时候进行压缩，目的是为了既不频繁压缩（影响性能），也不留存太多脏记录。

<br/>

<br/>

## 第六章：可靠的数据传递

可靠性是一个系统的属性，这种属性必须在系统设计之初就进行考虑。而Kafka支持不同程度的数据可靠性，这得取决于不同的使用场景。有的应用需要极强的数据可靠性，而有的应用则更倾向于性能和可扩展性…Kafka提供了非常灵活的配置和API来支持不同的用户场景。

也正是由于Kafka的灵活性，如果使用时不加以留意可能会导致问题。比如，你以为当前的系统是非常可靠的但实际却不然。下面我们来看下 Kafka 中的可靠性语义。

<br/>

### 可靠性保证

当我们讨论可靠性的时候，我们总会提到**保证**这个词语。可靠性保证是基础，我们基于这些基础之上构建我们的应用。比如关系型数据库的可靠性保证是ACID，也就是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。

Kafka中的可靠性保证有如下四点：

- 对于一个分区来说，它的消息是有序的。如果一个生产者向一个分区先写入消息A，然后写入消息B，那么消费者会先读取消息A再读取消息B。
- 当消息写入所有in-sync状态的副本后，消息才会认为**已提交（committed）**。这里的写入有可能只是写入到文件系统的缓存，不一定刷新到磁盘。生产者可以等待不同时机的确认，比如等待分区主副本写入即返回，后者等待所有in-sync状态副本写入才返回。
- 一旦消息已提交，那么只要有一个副本存活，数据不会丢失。
- 消费者只能读取到已提交的消息。

使用这些基础保证，我们构建一个可靠的系统，这时候需要考虑一个问题：究竟我们的应用需要多大程度的可靠性？可靠性不是无偿的，它与系统可用性、吞吐量、延迟和硬件价格息息相关，得此失彼。因此，我们往往需要做权衡，一味的追求可靠性并不实际。下面我们先来看下Kafka的复制机制是如何保证可靠性的，然后进一步讨论可靠性配置。

<br/>

### 复制

复制机制是Kafka可靠性保证的核心，

每一个Kafka的主题都会分成几个分区，每个分区都单独存放在磁盘上。Kafka保证分区内的消息顺序。每个分区可以有若干个相同的副本，其中一个为主副本。生产者发送消息到主副本，消费者从主副本中读取消息。其他副本需要保持与主副本同步，如果主副本不可用，那么这些同步中的副本其中之一会成为新的主副本。

一个副本被认为是in-sync（也就是及时同步）状态，当且仅当它为主副本，或者是保持如下行为的其他副本：

- 与Zookeeper的会话仍然活跃，即心跳间隔不超过6秒（时间可以配置）。
- 向主副本发起拉取数据请求间隔不超过10秒（时间可以配置）。
- 向主副本拉取的数据应当是10秒内产生的消息，也就是说不能拉取太老的消息。

如果一个副本不满足以上要求，那么它会变成out-of-sync状态直到它重新满足以上要求。

稍微有落后但仍然是in-sync状态的副本会拖慢生产者和消费者，因为它们需要等待所有in-sync状态的副本写入该消息。一旦某个副本变成out-of-sync状态，我们不需要等待它写入消息，这时候该副本不会造成任何性能影响，但同时由于in-sync状态副本减少，系统的冗余度也减少，数据丢失的可能性增大。

下面来看下实际中的具体应用及配置。

<br/>

### broker配置

Kafka中有三个broker的参数会影响数据可靠性。但就像其他的参数一样，这些参数可以在broker中设置（影响所有的主题），也可以在主题中设置（只影响该主题）。下面来看下这些参数。

<br/>

#### 复制因子（Replication Factor）

主题的复制因子配置参数为replication.factor；而对于自动创建的主题来说，broker中可以配置default.replication.factor来控制复制因子。

如果设置复制因子为N，那么我们能够容忍N-1个副本发生故障。因此更高的复制因子意味着更高的可用性以及数据可靠性；但另一方面，N个副本意味着需要存储N份相同的数据，需要N倍的存储空间。因此我们需要基于业务特点来决定复制因子。如果单个broker重启而导致主题不可用是可以接受的，那么复制因子为1就行。如果设置为2，那么我们可以容忍一个副本发生故障，但在此情况下系统只有1个副本运行，有丢失数据的风险。基于以上考虑，建议设置复制因子为3，对于大部分应用来说这种级别的容灾已经足够了，但特殊应用可能需要更高的冗余度，比如银行的关键数据存储会使用5个副本以防万一。

副本的位置也很重要。默认情况下，Kafka会将一个分区的不同副本放在不同的broker。但这仍然不够，如果所有的副本都在同一个机架上，那么无论复制因子为多少，该机架断电就会损失所有的数据。因此为了预防机架故障，在配置broker时建议配置broker.rack参数来指明机架信息，Kafka会利用此信息来将不同副本均衡到多个机架上。

<br/>

#### 有损主副本选举

unclean.leader.election.enable这个参数只能在broker级别设置，默认为true。这个参数有什么用？如前所述，如果分区的主副本不可用，那么in-sync副本中会选举出新的主副本，这样的选举是无损（clean）的，因为主副本切换没有导致数据丢失。但如果除了主副本外，没有in-sync副本存在呢？这种情况可能由于如下两种场景导致：

- 分区有多个副本，除了主副本外其他副本均发生故障，但我们仍然往主副本中写入消息，这会导致其他副本同步落后。如果主副本不幸也发生故障，而在故障恢复中其他副本先恢复，那么现在集群中只有落后的副本可用。
- 分区有多个副本，但由于网络原因，其他副本落后于主副本变成out-of-sync状态。假如主副本发生故障，集群中同样只有落后的副本可用。

对于这样的情况，我们需要从可用性与一致性之间作出选择：

- 如果我们不允许落后副本成为新的主副本，那么集群是不可用的直到我们启动老的主副本。在实际情况中，这可能不能快速恢复，而且也不一定能恢复。
- 如果我们允许落后副本成为新的主副本，那么丢失了部分已经提交的数据，而且导致消费者看到不一致的状态。

设置unclean.leader.election.enable为true，会允许落后副本成为新的主副本，这也是Kafka的默认设置。对于数据可靠性以及一致性非常重要的场景，用户可以设置为false。

<br/>

#### 最少in-sync副本数

对于最少in-sync副本数，主题与broker维度的参数都为min.insync.replicas。

上面可以看到，即便我们有多个副本，但最后可能只有一个副本（也就是主副本）是in-sync状态的。而且诡异的是，即便我们设置acks=all，也只有成功写入到一个副本中；也就是说，这里的all指的是所有in-sync状态副本，而不是所有副本。如果主副本故障，那么就面临上面说的问题，需要在一致性和可用性中做艰难取舍。

为了避免陷入这种情况，对于3个副本的分区来说，我们可以设置min.insync.replicas至少为2，这样可以保证数据至少写入了2份。如果系统中in-sync状态副本只剩下主副本，由于不满足min.insync.replicas，这时候生产者生产消息会失败，但消费者仍然可以读取之前的消息。这保证了一致性，牺牲了部分的可用性（但没有完全不可用）。

<br/>

### 可靠的生产者

即使我们使用最可靠的方式来配置broker以保证数据不丢失，但如果使用生产者的方式不对，那么仍然可能会造成丢失。

下面从两个场景来说明这个问题：

- 我们配置broker使用3个副本来进行冗余，并且禁止了有损主副本选举，这保证了一旦消息提交成功，那么消息将不会丢失。但是，假如我们配置生产者使用acks=1的确认方式，那么消息发送到主副本并且写入成功后，主副本返回结果，这时候生产者认为消息已经提交成功了。如果这时候主副本返回结果后就出现故障，并且没有把该消息同步到in-sync副本（in-sync副本允许有短暂的消息落后），那么in-sync副本会选举出新的主副本，而新的主副本并没有包含该消息。这样便出现生产者“认为”消息提交成功，但消息丢失的情况。
- 我们配置broker使用3个副本来进行冗余，并且禁止了有损主副本选举，而且生产者使用acks=all的配置。假如在写入消息时，主副本发生故障（并进行新主副本选举），那么broker会返回“主副本不可用”的异常。如果我们程序没有处理这个错误并进行重试，那么消息将会丢失。这样的数据丢失并不是由于broker本身可靠性导致的，因为broker就没有成功收到这个消息。

因此对于可靠性非常重要的系统来说，在使用生产者写入消息时需要注意如下两点：

- 使用正确的acks参数来满足业务的可靠性需要；
- 正确处理写入消息的异常信息。

下面来对不同的acks取值进行深入的讨论。

<br/>

#### 消息确认

生产者可以选择三种acks取值：

- acks=0：这种方式意味着，一旦生产者将消息通过网络传输成功，那么就认为消息已经写入到Kafka。假如这时候分区并不可用或者正在进行主副本选举，那么消息将会丢失，但生产者并不知晓。因为生产者在传输成功就进行后续处理了，没有关注Kafka的处理结果。当然，如果是消息序列化失败或者网卡发生故障，生产者还是能够得知这些错误的，因为这些错误发生在传输过程中。但这种方式也是性能最高的方式，虽然可能会导致消息丢失。
- acks=1：这种方式意味着，生产者传输消息到主副本，并等待主副本写入磁盘（注意，可能只是写入到文件系统缓存，不一定刷新到磁盘），如果写入完成则返回成功，否则返回失败。如果发送消息时分区正在进行主副本选举，那么生产者会收到主副本不可用异常，在收到此异常信息后，生产者可以进行重试来避免消息丢失。但在主副本发生故障时，那些没有及时同步到其他副本的消息会丢失。
- acks=all：这种方式意味着，生产者传输消息到主副本，并且在所有in-sync副本写入成功后主副本返回确认信息，否则返回错误。这个参数同时配合broker的min.insync.replica参数使用，能够实现“一旦返回成功，意味着最少写入N个副本”的语义，其中N为min.insync.replica的取值。如果生产者接收到错误，那么需要进行重试以防止消息丢失。这种方式也是最慢的处理方式，因为生产者需要等待所有in-sync副本写入成功。

下面对生产者遇到错误后的重试策略进行深入讨论。

<br/>

#### 配置生产者的重试机制

在使用生产者发送消息的时候，我们将会两类错误：

- 可恢复错误：对于这种错误，生产者内部会进行重试。比如说broker返回主副本不可用异常，当生产者收到此异常后会进行重试。
- 不可恢复错误：对于这种错误，即便生产者进行重试也不会成功，因此需要应用本身进行处理。比如说broker返回配置非法异常，当生产者收到此异常后进行重试也于事无补，需要应用自身进行处理。

如果你希望一条消息也不丢失，那么对于可恢复异常来说，可能会希望生产者一直重试直到成功。如果是因为主副本选举或者网络抖动而导致的异常，那么这种策略没什么问题。但如果网络一时半会恢复不了，我们也可以放弃重试并记录异常（比如记录日志、持久化到数据库等等），后续再进行处理。这取决于应用本身。注意到Kafka的跨机房复制工具（MirrorMaker）默认采取无限重试的策略，这是因为作为高可靠性的复制工具来说，它不应该丢失任何一条消息。

另外，生产者重试也可能会导致消息重复。假如消息发送到broker并且所有in-sync副本都写入成功，但在返回结果时网络发生故障，这时候生产者由于没收到回复认为消息没有发送成功，然后进行重试，这样便导致消息重复。异常处理和重试能够保证消息“最少一次”的语义，但无法保证“有且仅有一次”的语义（至少0.10.0版本Kafka如此）。应用本身如果需要实现“有且仅有一次”的语义，可以在消息中加入全局唯一标识符，这样在消费消息时可以进行去重。或者，应用生产幂等的消息，也就是说发送重复的消息没有影响，比如说“账户余额为110元”是幂等消息（因为发送多次也不会对账户造成影响），而“账户增加110元”则不是幂等消息（因为发送多次的结果并不一样）。

<br/>

#### 其他的错误处理

生产者的内部重试机制已经能解决大部分问题，但还有一些错误是需要应用进行处理的：

- 不可恢复异常（比如消息大小非法、权限认证失败）；
- 消息发送到broker之前发生的异常（比如序列化异常）；
- 生产者达到重试上限的异常，或者由于消息重试导致消息堆积最终内存溢出的异常。

对于这些异常，我们可以记录日志，持久化到数据库或者简单的抛弃。但如果说我们的处理方式仍然为不断重试，那么建议把这样的重试策略下沉到生产者内部重试机制。

<br/>

<br/>

### 可靠的消费者

上面讨论了如何使得生产者更可靠，现在来看下消息消费端的可靠处理方式。之前说到，当消息变成已提交状态（也就是写入到所有in-sync副本）后，它才能被消费端读取。这保证了消费者读取到的数据始终是一致的，为了达到高可靠，消费者需要保证在消费消息时不丢失数据。

在处理分区消息时，消费者一般的处理流程为：拉取批量消息，处理完成后提交位移，然后再拉取下一批消息。提交位移保证了当前消费者发生故障或重启时，其他消费者可以接着上一次的消息位移来进行处理。需要提醒的是，消费端丢失消息的一个主要原因为：消费者拉取消息后还没处理完就提交位移，一旦在消息处理过程中发生故障，新的消费者会从已提交的位移接着处理，导致发生故障时的消息丢失。

下面来看下消费端处理流程中的一些需要注意的细节。

<br/>

#### 重要的可靠性配置

如果希望设计一个高可靠的消费者，那么消费者中有4个重要的属性需要慎重考虑。

第一个属性是group.id，大概的作用是：如果有多个消费者拥有相同的group.id并且订阅相同的主题，那么每个消费者会负责消费一部分的消息。如果消费组内存在多个消费者，那么一个消费者发生故障那么其他消费者可以接替其工作，保证高可用。

第二个属性是auto.offset.reset，这个属性在如下场景中起作用：当消费者读取消息，Kafka中没有提交的位移（比如消费者所属的消费组第一次启动）或者希望读取的位移不合法（比如消费组曾经长时间下线导致位移落后）时，消费者如何处理？当设置为earliest，消费者会从分区的起始端开始读取，这可能会导致消费者重复处理消息，但也将消息丢失可能性降低到最小；当设置为latest，消费者会从分区末端开始读取，这会导致消息丢失可能性加大，但会降低消息重复处理的概率。

第三个属性是enable.auto.commit，这个属性需要慎重考虑，那就是：你希望消费者定期自动提交位移，还是应用手动提交位移？自动提交位移可以让应用在处理消息时不用实现提交位移的逻辑，并且如果我们是在poll循环中使用相同的线程处理消息，那么自动提交位移可以保证在消息处理完成后才提交位移。如果我们在poll循环中使用另外的线程处理消息，那么自动提交位移可能会导致提交还没完成处理的消息位移。

第四个属性是auto.commit.interval.ms，它与第三个属性有关。如果选择了自动提交位移，那么这个属性控制提交位移的时间间隔。默认值是5秒，通常来说降低间隔可以降低消息重复处理的可能性。

<br/>

#### 手动提交位移

如果我们选择手动提交位移，下面来根据不同场景来讨论如何实现更可靠的消费者。

**处理完消息后立即提交**

如果在poll循环中进行消息处理，并且处理完后提交位移，那么提交位移的实现方式非常简单。对于这种场景，可以考虑使用自动提交而不是手动提交。

**在处理消息过程中多次提交**

消费者拉取批量消息后处理消息时，在处理过程中可以使用手动提交位移方式来多次更新位移。这种方式可以使得消息重复处理可能性降低。不过在这个场景中，如果不加以注意，那么可能会提交上一次拉取的最大位移而不是当前已经处理的消息位移。

**重平衡**

在设计应用时，我们需要记得正确处理重平衡。当重平衡发生时，消费者当前处理的分区可能会被回收，我们需要记得在回收前提交位移。

**消费者的重试**

第一种处理方式是，我们提交已经处理成功的位移，然后将处理失败的消息存储到一个缓冲区，并不断进行重试处理这些消息。另外，在处理这些消息时可能poll循环仍然在继续，我们可以使用pause()方法来使得poll不会返回新的数据，这样使得重试更容易。

第二种处理方式是，我们把处理失败的消息写入到另外的主题，然后继续处理当前的消息。对于失败消息的主题，我们既可以使用同一个消费组进行处理，也可以使用不同的消费组进行处理。这种主题类似于其他消息系统使用的死信队列（dead-letter-queue）。

**持久化状态**

在某些场景下，我们可能需要在拉取消息时维护状态。比如，对于计算滑动平均数（moving average），我们每次拉取新消息时需要更新相应的平均数。当消费者重启时，我们不但需要从上一次提交的位移开始消费，同时还需要从相应的滑动平均数中恢复。一种处理方式是，我们提交位移时将滑动平均数写入到一个用于保存结果的主题，这样应用重启时可以获取上一次的处理结果。但由于Kafka不支持多操作的事务性，因此这种方式并不严谨。我们当然可以自己加以处理，但这个问题解决起来比较复杂，建议可以使用Kafka Streams这样的开源库。

**消息处理时间长**

某些应用拉取消息回来后处理消息时间比较长（比如依赖于一个阻塞服务或者进行复杂的计算），而某些版本的消费者如果长时间不poll消息会导致会话超时，因此使用这些版本的应用需要不断的拉取消息来发送心跳包到broker。一种常见的处理方式是，我们使用多线程来处理消息，然后当前线程调用pause()来使得既可以调用poll()而且消费者不会拉取新的消息；当消息处理完成后，再调用resume()来使得消费者恢复正常拉取逻辑。

**有且仅有一次的语义**

一种常见的处理方式为，我们使用支持唯一键的外部系统（比如关系型数据库、Elasticsearch等）来进行结果去重。我们可以自己实现唯一键并且在消息中加入此属性，也可以根据消息的主题、分区以及位移信息来生成唯一键。另外，如果该外部系统支持事务，那么我们可以在一个事务中同时保存消息处理结果和位移。消费者重启时可以从该系统中获取位移，并且使用seek()方法来开始从相应的位移开始消费。

<br/>

<br/>

### 验证系统的可靠性

当完成整个系统（生产者、broker、消费者）的高可靠设计后，我们下一步需要去验证设计和配置是否正确，验证工作可以分为三层：验证配置、验证应用和在线上监控应用。

<br/>

#### 验证Kafka配置

在验证配置时，建议将Kafka整体配置与业务逻辑分离，这样做有两大原因：

- 这样可以验证配置是否满足需求。
- 这样可以猜测Kafka系统行为并且进行验证。

<br/>

Kafka中有两个工具来协助我们完成配置验证，那就是org.apache.kafka.tools包中的VerifiableProducer和VerifiableConsumer。使用VerifiableProducer时，我们可以像配置一般的生产者一样配置acks和retries，并且可以配置消息生产速率；当运行VerifiableProducer起来后，它会打印出消息写入的结果。而VerifiableConsumer则可以进行写入消息的检查，它按序打印出消费的消息以及提交、重平衡等信息。

在使用前，我们先定义好测试场景，比如：

- 主副本选举：如果杀死主副本会怎么样？生产者和消费者多长时间才恢复正常？
- 控制器选举：控制器重启后系统多长时间才恢复正常?
- broker重启：逐一重启broker是否会导致消息丢失？
- 有损主副本选举：如果杀死所有的分区副本然后启动一个out-of-sync状态的副本会怎么样？如果在此场景下希望系统恢复那么配置是否正确？行为是否符合预期？

在定义好测试场景后，我们便可以启动VerifiableProducer和VerifiableConsumer，并且验证相应的场景。

<br/>

#### 验证应用

在验证完生产者、broker和消费者配置后，我们需要验证整体的业务逻辑，主要包含异常处理、消息提交、重平衡监听器以及其他Kafka客户端交互逻辑。在验证业务逻辑上，Kafka无法提供更多的支持，建议应用考虑多种故障场景下的逻辑验证：

- 客户端与broker断开连接；
- 主副本选举；
- broker重启；
- 消费者重启；
- 生产者重启。

对于每种场景，我们应该猜测预期的行为并进行结果验证。比如，在进行消费者重启的验证时，可能会预期消费者会短暂的暂停消费（由于重平衡），然后快速恢复，而且不会导致超过1000条记录重复处理。

<br/>

#### 线上监控可靠性

最后，我们需要实时监控线上系统的运行情况。

Kafka客户端（生产者/消费者）包含有监控状态和事件的JMX指标。对于生产者来说，最重要的两个指标为平均每个消息的错误率和重试率，这两个指标上升意味着系统出现了问题。另外，我们也需要关注生产者的日志，如果日志中出现“Got error produce response with correlation id 5689 on topic-partition [topic-1,3], retrying (two attempts left). Error: …”这样的异常信息并且日志中的重试次数为0，那么意味着该消息已经达到了重试次数的上限，基于此我们可能会希望提高重试次数的阈值。

对于客户端来说，最重要的指标为消息延迟（consumer lag）。这个指标反映了消费者当前拉取的消息与最新消息的差距，在理想情况下应该为0，但是因为我们拉取消息后需要进行处理，因此消息延迟不大也是正常的。由于消息延迟的上下波动是正常的，因此对于这个指标设置报警可能比较复杂，我们可以使用LinkedIn开源的[Burrow](https://github.com/linkedin/Burrow)来减轻开发量。

此外，为了跟踪消息的整体生产及处理流程，Kafka从0.10.0版本开始加入了消息生产的时间戳。为了验证消息在特定时间内被消费，我们需要在生产端和消费端同时记录消息数，然后消费端根据生产的时间戳来计算整体消息的延迟时间。这些数据需要有一个实时系统负责收集和监控，这样的系统实现起来会比较复杂，目前除了Confluent公司的商业化产品[Confluent Control Center](https://www.confluent.io/product/control-center/)外并没有开源的实现。

